---
title: "Final Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
```

### Introduction

### The Data

We will be using data that has extensive information on secondary school students in their math class. 

```{r}
data <- read.csv("data/student-mat.csv")


```
### Creation of New Variables

In order to provide more insight, we saw room to create informative variables based upon the given data. 

The given variables Medu and Fedu give information about the student's parents education history. Using this, we created a new variable "first_gen_college" that indicates if the student would be a first generation college student if they decided to pursue higher education. This will give more tangible and clear insight to how parental education impacts student's performance. 

```{r}
data <- data %>%
  mutate(first_gen_college = case_when(
    Medu < 4 & Fedu < 4 ~ "yes",
    TRUE ~"no"
  ))
```

Additionally, many variables are self reported ratings from the students on a scale of 1-5. We decided that instead factoring these variables so that scores of 1-3 would be "low" and scores of 4-5 would be "high" would be beneficial to our analysis as it would be more interpretable in context. 

```{r}
data <- data %>%
  mutate(famrel = case_when(
    famrel == 1 ~ "low",
    famrel == 2 ~ "low",
    famrel == 3 ~ "low",
    famrel == 4 ~ "high",
    famrel == 5 ~"high"
  ))

data <- data %>%
  mutate(freetime = case_when(
    freetime == 1 ~ "low",
    freetime == 2 ~ "low",
    freetime == 3 ~ "low",
    freetime == 4 ~ "high",
    freetime == 5 ~"high"
  ))

data <- data %>%
  mutate(goout = case_when(
    goout == 1 ~ "low",
    goout == 2 ~ "low",
    goout == 3 ~ "low",
    goout == 4 ~ "high",
    goout == 5 ~"high"
  ))

data <- data %>%
  mutate(Dalc = case_when(
    Dalc == 1 ~ "low",
    Dalc == 2 ~ "low",
    Dalc == 3 ~ "low",
    Dalc == 4 ~ "high",
    Dalc == 5 ~"high"
  ))

data <- data %>%
  mutate(Walc = case_when(
    Walc == 1 ~ "low",
    Walc == 2 ~ "low",
    Walc == 3 ~ "low",
    Walc == 4 ~ "high",
    Walc == 5 ~"high"
  ))

data <- data %>%
  mutate(health = case_when(
    health == 1 ~ "low",
    health == 2 ~ "low",
    health == 3 ~ "low",
    health == 4 ~ "high",
    health == 5 ~"high"
  ))
```

Additionally, using information from the famsup and internet variables, we created a variable called "stable_learning_env". If famsup is "yes" and internet is "yes", then stable_learning_env is "yes", otherwise "no".

```{r}
data <- data %>%
  mutate(stable_learning_env = case_when(
    internet =="yes" & famsup =="yes" ~"yes",
    TRUE ~"no"
  ))
```

Also, we created a new variable "high_freq_absent", which if absences >= 10 for a student, we considered them a highly frequent student.

```{r}
data <- data %>%
  mutate(high_freq_absent = case_when(
    absences >= 10 ~"yes",
    TRUE ~"no"
  ))
```

We also created a "failed" variable, which was "yes" if failures > 0, and "no" otherwise.

```{r}
data <- data %>%
  mutate(failed = case_when(
    failures > 0 ~"yes",
    TRUE ~"no"
  ))
```



### Exploratory Data Analysis 

```{r}
summary(data)
```

First, I will start off with univariate and bivariate plots of the response variable and key predictors I see being important. 

```{r}
data %>%
  filter(failed =="yes") %>%
  ggplot(aes(G3)) + 
  geom_histogram(stat = "count") +
  labs(title="Final Grade Distribution")

data %>%
  filter(failed =="yes") %>%
  ggplot(aes(G3)) + 
  geom_histogram(stat = "count") +
  labs(title="Final Grade Distribution")

data %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()

data %>%
  keep(is.character) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(stat="count")


```

Above we see that the response variable, G3, is pretty normally distributed, thus no transformation is necessary, 


```{r}
ggplot(data = data, aes(x = G3, y = first_gen_college, fill=first_gen_college)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = Walc, fill = Walc)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = famrel, fill = famrel)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y= sex, fill = sex)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = high_freq_absent, fill = high_freq_absent)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=failed, fill = failed)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=romantic, fill = romantic)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=internet, fill = internet)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=goout, fill = goout)) +
  geom_boxplot() 

```

From the initial explorations above, we can see a few possible trends. Students who had at least one of the following traits: failed a class previously, were a highly frequent absent student, frequently went out, without internet, were frequent drinkers on the weekend, were in romantic relationships, and were first generation students, on average had lower final grades than their counterparts.


```{r}
names(data)
```

```{r}
num_cols <- unlist(lapply(data, is.numeric))
quant_vars <- data[,num_cols]
cor(quant_vars)
#library(corr)
#quant_vars %>% correlate() %>% network_plot(min_cor=0.2)
```


## Splitting data into training and testing sets

```{r}
data <- data %>%
  mutate(ord_g3 = factor(G3, ordered=T)
  )
data$ord_g3
attach(data)
set.seed(3)
train_ind <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg <- -train_ind
training <- data[train_ind, ]
testing <- data[test_ind_neg, ]
training
testing

```


## Linear model

Modeling, diagnostics, predictions

```{r}
base_lm <- lm(G3 ~ . -G2 -G1 -ord_g3 -stable_learning_env, data)
summary(base_lm)

step.model <- stepAIC(base_lm, direction="both")
summary(step.model)
```

From fitting the base variables in a linear model, we can see that the variables sex, schoolsup, famsup, romantic, freetime, goout, absences, and failed are active. 

Based on the backwards stepwise regression model, it seems that the variables sex, Mjob, studytime, higher, romantic, freetime, goout, absences, first_gen_college, famsup, failed.

Based on these active variables, some interactions that we think could be significant are: schoolsup*failed, famsup*first_gen_college, higher*first_gen_college.

```{r}
activelm <- lm(G3 ~ (sex + schoolsup + romantic + freetime + goout + absences + failed + Mjob + studytime + higher + first_gen_college + famsup)^2)

summary(activelm)
```

From this, we can see that there seem to be significant interaction effects between sex and schoolsup, sex and first_gen_college, sex and Mjob, schoolsup and absences, schoolsup and studytime, schoolsup and first_gen_college, absences and failed, failed and first_gen_college, Mjob and studytime, Mjob and first_gen_college, studytime and famsup. 


## Multicategory ordinal logit model

Due to the way grades are assigned as values between 0 and 20, we would like to consider G3 as an ordered categorical variable with 21 levels. This would allow us to fit a multicategory ordinal logistic model to the data. 

We examine the EDA and active variables in the linear model to choose the predictors in our base model. 

Fitting the base model:

```{r}

require(foreign)
require(nnet)
require(ggplot2)
require(reshape2)
require(MASS)
require(Hmisc)

mod1 <- polr(ord_g3 ~ failed + high_freq_absent + romantic + internet + goout + first_gen_college + Walc + sex + schoolsup + famsup + absences + studytime + higher, data = training)
summary(mod1)
(ctable <- coef(summary(mod1)))
```

Calculate and store p-values:

```{r}
p1 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p1))

```
Confidence intervals for parameter estimates:

```{r}
(ci1 <- confint(mod1))

```

Analyzing the p-values and confidence intervals allows us to determine whether the coefficient estimates are significant. Based on these, failed, romantic, goout, first_gen_college, sex, schoolsup, famsup seem to be active. (Studytime is dubious, but we will include it in the next model)

Refitting a model with these predictors:

(Note: AIC is lower with studytime than without, and studytime seems significant.)

```{r}
mod2 <- polr(ord_g3 ~ failed + romantic + goout + first_gen_college + studytime + sex + schoolsup + famsup, data = training)
summary(mod2)
(ctable <- coef(summary(mod2)))
p2 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p2))
(ci2 <- confint(mod2))

```

AIC has decreased. 

Based on the p-values and confidence intervals, romantic does not seem to be significant. Let's try excluding it. 

Pared-down model again: 
```{r}
mod3 <- polr(ord_g3 ~ failed + goout + first_gen_college + sex + schoolsup + studytime, data = training)
summary(mod3)
(ctable <- coef(summary(mod3)))
p3 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p3))
(ci3 <- confint(mod3))

```
All predictors are significant, but AIC has increased compared to mod2.

Evaluating accuracy of the model for the training set:

```{r}
acc.ord3 <- predict(mod3, training)
ctable <- table(training$G3, acc.ord3)
round((sum(diag(ctable))/sum(ctable))*100,2)
ctable
```

Very terrible accuracy even for the training set.

What if we add interaction terms?

Let's base our interaction terms on the discussion for the linear model. 

```{r}
mod4 <- polr(ord_g3 ~ failed + goout + romantic +  first_gen_college + sex + schoolsup + sex*schoolsup + sex*first_gen_college + schoolsup * failed + schoolsup * studytime + schoolsup * first_gen_college + studytime*famsup, data = training)
summary(mod4)
(ctable <- coef(summary(mod4)))
p4 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p4))
(ci4 <- confint(mod4))
```
AIC has decreased significantly compared to the previous models without interaction terms, by nearly 20. However, in this model, sex, its interaction with schoolsup, and its interaction with first_gen_college all seem to be insignificant. The interaction between studytime and famsup and failed and schoolsup do not seem significant either, so let us remove it to pare down the model: 

```{r}
mod5 <- polr(ord_g3 ~ failed + goout + romantic + schoolsup + first_gen_college + schoolsup * studytime + schoolsup * first_gen_college, data = training)
summary(mod5)
(ctable <- coef(summary(mod5)))
p5 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p5))
(ci5 <- confint(mod5))

```
This has resulted in an increase in the AIC, which is still lower than the first three models. 

Let's check the accuracy of this model with interaction terms:

```{r}
acc.ord4 <- predict(mod4, training)
ctable <- table(training$G3, acc.ord4)
round((sum(diag(ctable))/sum(ctable))*100,2)
ctable

```
The accuracy is even lower than mod3, at only 19.94% for the training set.


Checking on testing set:

```{r}
pred.ord3 <- predict(mod3, testing)
ctable <- table(testing$G3, pred.ord3)
round((sum(diag(ctable))/sum(ctable))*100,2)

pred.ord4 <- predict(mod4, testing)
ctable <- table(testing$G3, pred.ord4)
round((sum(diag(ctable))/sum(ctable))*100,2)


pred.ord5 <- predict(mod5, testing)
ctable <- table(testing$G3, pred.ord5)
round((sum(diag(ctable))/sum(ctable))*100,2)


```
Highly inaccurate model, not a good fit for the data.


## Trees etc


