---
title: "Final Project"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
require(foreign)
require(nnet)
require(reshape2)
require(Hmisc)
library(randomForest)
library(tree)
library(rpart)

```

### Introduction

### The Data

We will be using data that has extensive information on secondary school students in their math class. 

```{r}
data <- read.csv("data/student-mat.csv")


```
### Creation of New Variables

In order to provide more insight, we saw room to create informative variables based upon the given data. 

The given variables Medu and Fedu give information about the student's parents education history. Using this, we created a new variable "first_gen_college" that indicates if the student would be a first generation college student if they decided to pursue higher education. This will give more tangible and clear insight to how parental education impacts student's performance. 

```{r}
data <- data %>%
  mutate(first_gen_college = case_when(
    Medu < 4 & Fedu < 4 ~ "yes",
    TRUE ~"no"
  ))
data[["first_gen_college"]] <- as.factor(data[["first_gen_college"]])
```

Additionally, many variables are self reported ratings from the students on a scale of 1-5. We decided that instead factoring these variables so that scores of 1-3 would be "low" and scores of 4-5 would be "high" would be beneficial to our analysis as it would be more interpretable in context. 

```{r}
data <- data %>%
  mutate(famrel = case_when(
    famrel == 1 ~ "low",
    famrel == 2 ~ "low",
    famrel == 3 ~ "low",
    famrel == 4 ~ "high",
    famrel == 5 ~"high"
  ))

data <- data %>%
  mutate(freetime = case_when(
    freetime == 1 ~ "low",
    freetime == 2 ~ "low",
    freetime == 3 ~ "low",
    freetime == 4 ~ "high",
    freetime == 5 ~"high"
  ))

data <- data %>%
  mutate(goout = case_when(
    goout == 1 ~ "low",
    goout == 2 ~ "low",
    goout == 3 ~ "low",
    goout == 4 ~ "high",
    goout == 5 ~"high"
  ))

data <- data %>%
  mutate(Dalc = case_when(
    Dalc == 1 ~ "low",
    Dalc == 2 ~ "low",
    Dalc == 3 ~ "low",
    Dalc == 4 ~ "high",
    Dalc == 5 ~"high"
  ))

data <- data %>%
  mutate(Walc = case_when(
    Walc == 1 ~ "low",
    Walc == 2 ~ "low",
    Walc == 3 ~ "low",
    Walc == 4 ~ "high",
    Walc == 5 ~"high"
  ))

data <- data %>%
  mutate(health = case_when(
    health == 1 ~ "low",
    health == 2 ~ "low",
    health == 3 ~ "low",
    health == 4 ~ "high",
    health == 5 ~"high"
  ))

data[["sex"]] <- as.factor(data[["sex"]])
data[["address"]] <- as.factor(data[["address"]])
data[["famsize"]] <- as.factor(data[["famsize"]])
data[["Pstatus"]] <- as.factor(data[["Pstatus"]])
data[["Mjob"]] <- as.factor(data[["Mjob"]])
data[["Fjob"]] <- as.factor(data[["Fjob"]])
data[["reason"]] <- as.factor(data[["reason"]])
data[["guardian"]] <- as.factor(data[["guardian"]])
data[["schoolsup"]] <- as.factor(data[["schoolsup"]])
data[["famsup"]] <- as.factor(data[["famsup"]])
data[["paid"]] <- as.factor(data[["paid"]])
data[["activities"]] <- as.factor(data[["activities"]])
data[["nursery"]] <- as.factor(data[["nursery"]])
data[["higher"]] <- as.factor(data[["higher"]])
data[["internet"]] <- as.factor(data[["internet"]])
data[["romantic"]] <- as.factor(data[["romantic"]])
data[["famrel"]] <- as.factor(data[["famrel"]])
data[["freetime"]] <- as.factor(data[["freetime"]])
data[["goout"]] <- as.factor(data[["goout"]])
data[["Dalc"]] <- as.factor(data[["Dalc"]])
data[["Walc"]] <- as.factor(data[["Walc"]])
data[["health"]] <- as.factor(data[["health"]])
```

Additionally, using information from the famsup and internet variables, we created a variable called "stable_learning_env". If famsup is "yes" and internet is "yes", then stable_learning_env is "yes", otherwise "no".

```{r}
data <- data %>%
  mutate(stable_learning_env = case_when(
    internet =="yes" & famsup =="yes" ~"yes",
    TRUE ~"no"
  ))
data[["stable_learning_env"]] <- as.factor(data[["stable_learning_env"]])
```

Also, we created a new variable "high_freq_absent", which if absences >= 10 for a student, we considered them a highly frequent student.

```{r}
data <- data %>%
  mutate(high_freq_absent = case_when(
    absences >= 10 ~"yes",
    TRUE ~"no"
  ))
data[["high_freq_absent"]] <- as.factor(data[["high_freq_absent"]])
```

We also created a "failed" variable, which was "yes" if failures > 0, and "no" otherwise.

```{r}
data <- data %>%
  mutate(failed = case_when(
    failures > 0 ~"yes",
    TRUE ~"no"
  ))
data[["failed"]] <- as.factor(data[["failed"]])
```



### Exploratory Data Analysis 

```{r}
summary(data)
```

First, I will start off with univariate and bivariate plots of the response variable and key predictors I see being important. 

```{r}
data %>%
  filter(failed =="yes") %>%
  ggplot(aes(G3)) + 
  geom_histogram(stat = "count") +
  labs(title="Final Grade Distribution")

data %>%
  filter(failed =="yes") %>%
  ggplot(aes(G3)) + 
  geom_histogram(stat = "count") +
  labs(title="Final Grade Distribution")

data %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()

data %>%
  keep(is.character) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(stat="count")


```

Above we see that the response variable, G3, is pretty normally distributed, thus no transformation is necessary, 


```{r}
ggplot(data = data, aes(x = G3, y = first_gen_college, fill=first_gen_college)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = Walc, fill = Walc)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = famrel, fill = famrel)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y= sex, fill = sex)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = high_freq_absent, fill = high_freq_absent)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=failed, fill = failed)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=romantic, fill = romantic)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=internet, fill = internet)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=goout, fill = goout)) +
  geom_boxplot() 

```

From the initial explorations above, we can see a few possible trends. Students who had at least one of the following traits: failed a class previously, were a highly frequent absent student, frequently went out, without internet, were frequent drinkers on the weekend, were in romantic relationships, and were first generation students, on average had lower final grades than their counterparts.


```{r}
names(data)
```

```{r}
num_cols <- unlist(lapply(data, is.numeric))
quant_vars <- data[,num_cols]
cor(quant_vars)
#library(corr)
#quant_vars %>% correlate() %>% network_plot(min_cor=0.2)
```


## Creating variables for an ordinal final grade, 6-category final grade, and binary final grade

We'd like to examine final grades in multiple ways. The first is as a continuous numerical variable as G3 is.

The second is final grades as an ordered factor variable in order to perform multicategory ordinal logit modeling to see if we could improve fit and predictive power. However, this was unsuccessful.

```{r}
data <- data %>%
  mutate(ord_g3 = factor(G3, ordered=T)
  )

```
The third is final grades as a 6-category ordered factor variable according to the Portuguese education system's classifications. We believe this could address some of the outliers and abnormality in the data (for example, many students received 0's, but no one received a 1, 2, or 3).

```{r}
library(car)
data <- data %>%
  mutate(cat_g3 = case_when(
    G3 == 0 ~ "Poor",
    G3 <= 9 ~ "Weak",
    G3 <= 13 ~ "Sufficient",
    G3 <= 15 ~ "Good",
    G3 <= 17 ~"Very Good",
    G3 <= 20 ~ "Excellent"
  ))
data <- data %>%
  mutate(cat_g3 = factor(cat_g3, levels=c("Poor", "Weak", "Sufficient", "Good", "Very Good", "Excellent"), ordered=TRUE))

```

The fourth is final grades as a binary factor variable. This is done based on the previous categories in the Portuguese classification system. If the student receives a "poor" or "weak" grade, or G3 < 10, this is considered a "low" grade. If the student received a "sufficient" "good" "very good" or "excellent" grade, this is a high grade. 

```{r}
data <- data %>%
  mutate(pf = case_when(
    G3 >= 10 ~ "high",
    G3 < 10 ~ "low"
  ))
data <- data %>%
  mutate(pf = factor(pf, levels=c("high", "low"), ordered = FALSE))

```

## Splitting data into training and testing sets

```{r}
attach(data)
set.seed(3)
train_ind <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg <- -train_ind
training <- data[train_ind, ]
testing <- data[test_ind_neg, ]

```


## Linear model


```{r}
base_lm <- lm(G3 ~ . -G2 -G1 -ord_g3 -cat_g3 -pf, data=training)
vif(base_lm)
summary(base_lm)

```
From the vif, it is easy to see that stable_learning_env and famsup have high VIF values. This is likely because famsup and was used to create stable_learning_env. Failures and failed also have higher VIF values, likely because failures was used to create failed. Because we believe that failures is much more explanatory than failed, we will choose to include failures in the model. In order to combat multicollinearity and increase interpretability, we will exclude Medu and Fedu as well from the model, as these were used to create first_gen_college. 

We will then perform stepwise selection.

```{r}
base_lm1 <- lm(G3 ~ . -G2 -G1 -ord_g3 -cat_g3 -pf -famsup -failed -Medu -Fedu, data=training)
vif(base_lm1)
summary(base_lm1)
step.model <- stepAIC(base_lm1, direction="both")
summary(step.model)


```
The new base model had Multiple R-squared:  0.3105,	Adjusted R-squared:  0.2687. It had low VIF values for all predictors.

The model chosen by stepwise selection has Multiple R-squared:  0.3214,	Adjusted R-squared:  0.2753.

Based on the stepwise regression model, we can see that the variables sex, studytime, failures, schoolsup, romantic, internet, freetime, goout, absences, first_gen_college, stable_learning_environment seem to be significant active predictors. 



Based on these active variables, some interactions that we think could be significant are: schoolsup*failed, famsup*first_gen_college, higher*first_gen_college. Let us fit an active model with all interaction effects.

```{r}
activelm <- lm(G3 ~ (sex + studytime + failures + schoolsup + internet + romantic + freetime + 
    goout + absences + first_gen_college + stable_learning_env)^2, data=training)

summary(activelm)
```
Significant interactions exist between failures and absences, first_gen_college and failures, absences and stable_learning_env, schoolsup and absences, schoolsup and first_gen_college, sex and first_gen_college, sex and failures, studytime and schoolsup. 
Interestingly, in this model, the most active predictors that are not interaction terms are first_gen_college and stable_learning_env.

Fitting a pared-down active model with interaction effects:

```{r}
inter_lm <- lm(G3 ~ first_gen_college + stable_learning_env + failures * absences + first_gen_college*failures + absences*stable_learning_env + schoolsup * absences + schoolsup*first_gen_college + sex*first_gen_college + sex*failures + studytime * schoolsup, data = training)
summary(inter_lm)
AIC(inter_lm)
```

Unfortunately even with interaction effects, the Multiple R-squared:  0.3049,	Adjusted R-squared:  0.2701 and AIC is 1792.389. T

Using the model on the testing set:

```{r}
pred.lm <- predict(inter_lm, testing)
mse_test <- mean((pred.lm - testing$G3)^2)
mse_test
```
Test MSE of 16.7167.


### Regression random forest

The linear model did not seem a good fit to the data. Let us try a regression random forest. Because we would prefer simpler categories in this case, we will exclude variables that have been recoded as stable_learning_env and first_gen_college. We will also include failed instead of failures.

```{r}
library(randomForest)
reg.rf <- randomForest(G3 ~ . -G1 -G2 -G3 -ord_g3 -pf -cat_g3 -famsup -internet -failures -Medu -Fedu, data=training, mtry=3,
                         importance=TRUE, na.action=na.omit)
print(reg.rf)
importance(reg.rf)
varImpPlot(reg.rf)
yhat_rf <- predict(reg.rf, newdata = testing)
mse_test.rf <- mean((yhat_rf - testing$G3)^2)

mse_test.rf
```
Improved test MSE compared to the linear model. test MSE = 13.90083 24.94% variation explained; mean of squared residuals is 16.5.

A pared-down random forest fit with the most important predictors according to Node purity and % increase in MSE.

```{r}
reg.rf1 <- randomForest(G3 ~ failed + absences + schoolsup + first_gen_college + age + studytime + Pstatus + famsize + guardian + freetime + Mjob + romantic + paid + sex + goout, data=training, mtry=3,
                         importance=TRUE, na.action=na.omit)
print(reg.rf1)
importance(reg.rf1)
varImpPlot(reg.rf1)
yhat_rf1 <- predict(reg.rf1, newdata = testing)
mse_test.rf1 <- mean((yhat_rf1 - testing$G3)^2)

mse_test.rf1

```
Test MSE of 14.39569; 30.49% of var explained by model; mean of squared residuals: 15.28173.

Overall, the random forest on regression has improved Test MSE compared to linear modeling, but still has a relatively poor fit. This indicates that perhaps considering G3 as a continuous response variable is inadequate to examine relationships between final grades and other variables.

The 4 most important factors seem to be:
failed
absences
schoolsup
first_gen_college

## Multicategory ordinal logit model

Due to the way grades are assigned as values between 0 and 20, we would like to consider G3 as an ordered categorical variable with 21 levels. This would allow us to fit a multicategory ordinal logistic model to the data. 

We examine the EDA and active variables in the linear model to choose the predictors in our base model. 

Fitting the base model:

```{r}


mod <-polr(ord_g3 ~ . -G1 -G2 -G3 -cat_g3 -pf, data = training)
summary(mod)
acc.ord <- predict(mod, training)
ctable <- table(training$G3, acc.ord)
round((sum(diag(ctable))/sum(ctable))*100,2)
ctable

mod1 <- polr(ord_g3 ~ failed + high_freq_absent + romantic + internet + goout + first_gen_college + Walc + sex + schoolsup + famsup + absences + studytime + higher, data = training)
summary(mod1)
(ctable <- coef(summary(mod1)))
```

Calculate and store p-values:

```{r}
p1 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p1))

```
Confidence intervals for parameter estimates:

```{r}
(ci1 <- confint(mod1))

```

Analyzing the p-values and confidence intervals allows us to determine whether the coefficient estimates are significant. Based on these, failed, romantic, goout, first_gen_college, sex, schoolsup, famsup, studytime seem to be active. (Studytime is dubious, but we will include it in the next model)

Refitting a model with these predictors:


```{r}
mod2 <- polr(ord_g3 ~ failed + romantic + goout + first_gen_college + studytime + sex + schoolsup + famsup, data = training)
summary(mod2)
(ctable <- coef(summary(mod2)))
p2 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p2))
(ci2 <- confint(mod2))

```

AIC has decreased. 

Based on the p-values and confidence intervals, romantic does not seem to be significant. Let's try excluding it. 

Pared-down model again: 
```{r}
mod3 <- polr(ord_g3 ~ failed + goout + first_gen_college + sex + schoolsup + studytime, data = training, Hess=TRUE)
summary(mod3)
(ctable <- coef(summary(mod3)))
p3 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p3))
(ci3 <- confint(mod3))

```
All predictors are significant, but AIC has increased compared to mod2.

Evaluating accuracy of the model for the training set:

```{r}
acc.ord3 <- predict(mod3, training)
ctable <- table(training$G3, acc.ord3)
round((sum(diag(ctable))/sum(ctable))*100,2)
ctable
```

Very terrible accuracy even for the training set.

What if we add interaction terms?

Let's base our interaction terms on the discussion for the linear model. 

```{r}
mod4 <- polr(ord_g3 ~ failed + goout + romantic +  first_gen_college + sex + schoolsup + sex*schoolsup + sex*first_gen_college + schoolsup * failed + schoolsup * studytime + schoolsup * first_gen_college + studytime*famsup, data = training)
summary(mod4)
(ctable <- coef(summary(mod4)))
p4 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p4))
(ci4 <- confint(mod4))
```
AIC has decreased significantly compared to the previous models without interaction terms, by nearly 20. However, in this model, sex, its interaction with schoolsup, and its interaction with first_gen_college all seem to be insignificant. The interaction between studytime and famsup and failed and schoolsup do not seem significant either, so let us remove it to pare down the model: 

```{r}
mod5 <- polr(ord_g3 ~ failed + goout + romantic + schoolsup + first_gen_college + schoolsup * studytime + schoolsup * first_gen_college, data = training)
summary(mod5)
(ctable <- coef(summary(mod5)))
p5 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p5))
(ci5 <- confint(mod5))

```
This has resulted in an increase in the AIC, which is still lower than the first three models. 

Let's check the accuracy of this model with interaction terms:

```{r}
acc.ord4 <- predict(mod4, training)
ctable <- table(training$G3, acc.ord4)
round((sum(diag(ctable))/sum(ctable))*100,2)
ctable

```
The accuracy is even lower than mod3, at only 19.94% for the training set.


Checking on testing set:

```{r}
pred.ord3 <- predict(mod3, testing)
ctable <- table(testing$G3, pred.ord3)
round((sum(diag(ctable))/sum(ctable))*100,2)

pred.ord4 <- predict(mod4, testing)
ctable <- table(testing$G3, pred.ord4)
round((sum(diag(ctable))/sum(ctable))*100,2)


pred.ord5 <- predict(mod5, testing)
ctable <- table(testing$G3, pred.ord5)
round((sum(diag(ctable))/sum(ctable))*100,2)


```
Accuracy rates are even lower, at 8.86%, 11.39%, and 10.13%. 

Highly inaccurate model, not a good fit for the data.


## 6-category grades modeling

```{r}
set.seed(3)
train_ind <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg <- -train_ind
ftrain <- data[train_ind, ]
ftest <- data[test_ind_neg, ]

```

Trying out a multicat ordinal logit on this:

```{r}
mod6 <- polr(cat_g3 ~ failed + goout + romantic + schoolsup + first_gen_college + schoolsup * studytime + schoolsup * first_gen_college, data = ftrain)
summary(mod6)
(ctable <- coef(summary(mod6)))
p6 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p6))
(ci5 <- confint(mod6))


acc.ord6 <- predict(mod6, ftest)
ctable <- table(ftest$cat_g3, acc.ord6)
ctable
?diag()
class_accuracy <- sum(diag(ctable))/length(ftest$cat_g3)
class_accuracy
```
Still not very accurate for the training

To determine which predictors, satisfy the proportional odds assumption, we will run a test.
```{r}
# recode cat_g3 as numbers 1 to 6
train_vglm <- ftrain %>% 
  mutate(cat_g3 = as.ordered(ifelse(cat_g3 == "Poor", 1, 
                     ifelse(cat_g3 == "Weak", 2,
                        ifelse(cat_g3 == "Sufficient", 3, 
                           ifelse(cat_g3 == "Good", 4,
                              ifelse(cat_g3 == "Very Good", 5, 6)))))))
test_vglm <- ftest %>% 
  mutate(cat_g3 = as.ordered(ifelse(cat_g3 == "Poor", 1, 
                     ifelse(cat_g3 == "Weak", 2,
                        ifelse(cat_g3 == "Sufficient", 3, 
                           ifelse(cat_g3 == "Good", 4,
                              ifelse(cat_g3 == "Very Good", 5, 6)))))))

sf <- function(y) {
  c('Y>=1' = qlogis(mean(y >= 1)),
    'Y>=2' = qlogis(mean(y >= 2)),
    'Y>=3' = qlogis(mean(y >= 3)),
    'Y>=4' = qlogis(mean(y >= 4)),
    'Y>=5' = qlogis(mean(y >= 5)),
    'Y>=6' = qlogis(mean(y >= 6)))
}

require(Hmisc)
cat_table <- with(train_vglm, summary(as.numeric(cat_g3) ~ failed + goout + romantic + first_gen_college + 
                                        studytime + sex + guardian,
                                  fun = sf))
cat_table
train_vglm

?summary()
```

```{r}
glm(I(as.numeric(cat_g3) >= 2) ~ failed, family="binomial", data = train_vglm)
glm(I(as.numeric(cat_g3) >= 3) ~ failed, family="binomial", data = train_vglm)
glm(I(as.numeric(cat_g3) >= 4) ~ failed, family="binomial", data = train_vglm)
glm(I(as.numeric(cat_g3) >= 5) ~ failed, family="binomial", data = train_vglm)
glm(I(as.numeric(cat_g3) >= 6) ~ failed, family="binomial", data = train_vglm)

cat_table[,7] <- cat_table[,7] - cat_table[,3]
cat_table[,6] <- cat_table[,6] - cat_table[,3]
cat_table[,5] <- cat_table[,5] - cat_table[,3]
cat_table[,4] <- cat_table[,4] - cat_table[,3]
cat_table[,3] <- cat_table[,3] - cat_table[,3]
cat_table

plot(cat_table, which = 1:6, pch = 1:6, xlab = "logit", main = "Parallel Slopes Test", xlim = range(cat_table[,3:7]))

```
From this plot, it appears that only `goout` and `first_gen_college` satisfy the proportional odds assumption. 

```{r}
library(VGAM)
fitty <- vglm(cat_g3 ~ failed + absences + goout + sex + guardian + romantic + schoolsup + first_gen_college + schoolsup * studytime + schoolsup * first_gen_college, family = cumulative(parallel=TRUE~goout + first_gen_college + sex + guardian), data = ftrain)
summary(fitty)

vglm_pred_probs <- predict(fitty, ftest, type = "response")
vglm_pred_g3 <- colnames(vglm_pred_probs)[apply(vglm_pred_probs,1,which.max)]
vglm_pred_probs
tab <- table(ftest$cat_g3, vglm_pred_g3)
tab
```


Random forest:

```{r}
rf.cat<-randomForest(cat_g3~. -G1 -G2 -G3 -ord_g3 -pf -famsup -internet -Medu -Fedu,data = ftrain, mtry = 3, ntree=50, importance=TRUE) 
print(rf.cat)
importance(rf.cat)
varImpPlot(rf.cat)
rf.acc<- predict(rf.cat, ftrain, type = 'class')
t<-table(predictions=rf.acc, actual=ftrain$cat_g3)
t
sum(diag(t))/sum(t)

```

Very fitted model with accuracy for training data >99%.

Let's see what the accuracy rate for the testing set is:

```{r}
rf.pred<- predict(rf.cat, ftest, type = 'class')
t<-table(predictions=rf.pred, actual=ftest$cat_g3)
t
sum(diag(t))/sum(t)

```
43.03% accuracy, which is an improvement.


Let's choose the most important variables, as well as interaction effects we believe to be important based on previous exploration:

```{r}
rf.cat1<-randomForest(cat_g3~failures + absences + sex + Walc + Fjob +goout + schoolsup + first_gen_college + guardian + Walc,data = ftrain, ntree=50, importance=TRUE) 
print(rf.cat1)
importance(rf.cat1)
varImpPlot(rf.cat1)
rf.acc<- predict(rf.cat1, ftrain, type = 'class')
t<-table(predictions=rf.acc, actual=ftrain$cat_g3)
t
sum(diag(t))/sum(t)

```
54.75% OOB estimate of error rate and 83.5% accuracy rate for the training data.

```{r}
rf.pred1<- predict(rf.cat1, ftest, type = 'class')
t<-table(predictions=rf.pred1, actual=ftest$cat_g3)
t
sum(diag(t))/sum(t)

```
37.97% Accuracy, which is less than the full RF model.


The RF models indicate that for grade categorization, the most important variables are absences, failed, guardian, studytime, Mjob and Fjob, schoolsup, age, goout, first_gen_college (not in that order).  


## Modeling for low-high grades 

Considering final grades as a continuous variable and ordinal categorical variable gave poor results. Therefore, we'd like to model a binary variable that indicates whether the student has a high grade (grade >= 10) or low grade (<10). 

```{r}
set.seed(3)
train_ind1 <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg1 <- -train_ind1
ftrain1 <- data[train_ind1, ]
ftest1 <- data[test_ind_neg1, ]

```

## Fitting a decision tree on pass-fail

```{r}

data[["pf"]] <- as.factor(data[["pf"]])
training[["pf"]] <- as.factor(training[["pf"]])
testing[["pf"]] <- as.factor(testing[["pf"]])
treepf <- tree(pf ~ . -G1 -G2 -G3 -ord_g3 -failures -reason -health -age -nursery -ord_g3, data=training)
treepf
summary(treepf)
plot(treepf)
text(treepf, pretty = 0)
```

### Initial Tree Diagnostic

```{r}
tree.pred <- predict(treepf, testing, type = "class")
table(tree.pred, testing$pf)
sum(diag(table(tree.pred, testing$pf)))/79
```
Misclassification rate: 0.38. This can likely be decreased with other methods- using all variables likely overfits.

###Pruning

```{r}
set.seed(3)
cv.pf <- cv.tree(treepf, FUN = prune.misclass)
names(cv.pf)
cv.pf

par(mfrow = c(1,2))
plot(cv.pf$size, cv.pf$dev, type = "b")
plot(cv.pf$k, cv.pf$dev, type = "b")
```
```{r}
prune.pf <- prune.misclass(treepf, best = 3)
plot(prune.pf)
text(prune.pf, pretty = 0)

prune.short <- prune.misclass(treepf, best = 2)
plot(prune.short)
text(prune.short, pretty = 0)


treepred2 <- predict(prune.pf, testing, type = "class")
table(treepred2, testing$pf)
sum(diag(table(treepred2, testing$pf)))/79

treepred3 <- predict(prune.short, testing, type = "class")
table(treepred3, testing$pf)
sum(diag(table(treepred3, testing$pf)))/79
```
Misclassification rateL .32.

### Bagging
```{r}
library(randomForest)
set.seed(1)
bag.pf <- randomForest(pf ~ . -G1 -G2 -G3 -ord_g3 -failures -reason -health -age -nursery -ord_g3, data=training, mtry = 28, importance = TRUE, ntree = 75)
bag.pf
```
```{r}
yhat.bag <- predict(bag.pf, testing)
plot(yhat.bag, testing$pf)
table(yhat.bag, testing$pf)
sum(diag(table(yhat.bag, testing$pf)))/79
```

### Boosting
```{r}
library(gbm)
attach(data)
data[["pf_factor"]] <- as.factor(data[["pf"]])
data[["pf_bin"]] <- as.numeric(data[["pf_factor"]])-1
training[["pf_factor"]] <- as.factor(training[["pf"]])
training[["pf_bin"]] <- as.numeric(training[["pf_factor"]])-1
testing[["pf_factor"]] <- as.factor(testing[["pf"]])
testing[["pf_bin"]] <- as.numeric(testing[["pf_factor"]])-1

set.seed(1)
boost.pf <- gbm(pf_bin ~ . -pf_factor -pf -school -G1 -G2 -G3 -ord_g3 -failures -reason -health -age -nursery -ord_g3, data = training,
                    distribution = "bernoulli", n.trees = 500,
                    interaction.depth = 2)

summary(boost.pf)

```

```{r}
predboost1 <- predict(boost.pf, testing,
                      n.trees = 500)
table(predboost1, testing$pf_bin)
```
### Lower interaction depth
```{r}
boost.pf1 <- gbm(pf_bin ~ . -pf_factor -pf -school -G1 -G2 -G3 -ord_g3 -failures -reason -health -age -nursery -ord_g3, data = training,
                    distribution = "bernoulli", n.trees = 500,
                    interaction.depth = 1)
summary(boost.pf1)
predboost1 <- predict(boost.pf, testing,
                      n.trees = 500)
table(predboost1, testing$pf_bin)
```
Not much difference.



### Fitting random forest on low-high binary

Fitting with ALL predictors: 

```{r}
rf.bin<-randomForest(pf~. -G1 -G2 -G3 -ord_g3 - cat_g3 -Medu -Fedu -failures -famsup,data = ftrain1,mtry=3, ntree=50, importance=TRUE) 
print(rf.bin)
importance(rf.bin)
varImpPlot(rf.bin)
rf.acc<- predict(rf.bin, ftrain1, type = 'class')
t<-table(predictions=rf.acc, actual=ftrain1$pf)
t
sum(diag(t))/sum(t)

```
Predictions on testing set:
```{r}
rf.pred2<- predict(rf.bin, ftest1, type = 'class')
t<-table(predictions=rf.pred2, actual=ftest1$pf)
t
sum(diag(t))/sum(t)

```
68.35443% accuracy rate.

Finding the best random forest model by including important predictors:

```{r}
rf.bin1<-randomForest(pf~failed + absences+ guardian + studytime + goout + schoolsup + first_gen_college + Walc + famsup,data = ftrain1,mtry=3, ntree=50, importance=TRUE) 
print(rf.bin1)
importance(rf.bin1)
varImpPlot(rf.bin1)
rf.acc1<- predict(rf.bin1, ftrain1, type = 'class')
t<-table(predictions=rf.acc1, actual=ftrain1$pf)
t
sum(diag(t))/sum(t)

```
The pared-down model has an 00B estimate of error rate of 25.95% and a training set prediction accuracy rate of 90.19%.

Predictions on testing set:

```{r}
rf.pred3<- predict(rf.bin1, ftest1, type = 'class')
t<-table(predictions=rf.pred3, actual=ftest1$pf)
t
sum(diag(t))/sum(t)

```

70.88608% prediction accuracy rate, which is the highest achieved with RF.

Overall the random-forests for pass-fail indicate that the most important factors affecting whether the student gets a low or high final grade are failed, absences, guardian, studytime, goout, schoolsup, first_gen_college.


