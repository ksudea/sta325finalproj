---
title: "Final Project"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(broom)
library(stringr)
library(knitr)
library(nnet)
library(ggplot2)
library(MASS)
```

### Introduction

### The Data

We will be using data that has extensive information on secondary school students in their math class. 

```{r}
data <- read.csv("data/student-mat.csv")


```
### Creation of New Variables

In order to provide more insight, we saw room to create informative variables based upon the given data. 

The given variables Medu and Fedu give information about the student's parents education history. Using this, we created a new variable "first_gen_college" that indicates if the student would be a first generation college student if they decided to pursue higher education. This will give more tangible and clear insight to how parental education impacts student's performance. 

```{r}
data <- data %>%
  mutate(first_gen_college = case_when(
    Medu < 4 & Fedu < 4 ~ "yes",
    TRUE ~"no"
  ))
```

Additionally, many variables are self reported ratings from the students on a scale of 1-5. We decided that instead factoring these variables so that scores of 1-3 would be "low" and scores of 4-5 would be "high" would be beneficial to our analysis as it would be more interpretable in context. 

```{r}
data <- data %>%
  mutate(famrel = case_when(
    famrel == 1 ~ "low",
    famrel == 2 ~ "low",
    famrel == 3 ~ "low",
    famrel == 4 ~ "high",
    famrel == 5 ~"high"
  ))

data <- data %>%
  mutate(freetime = case_when(
    freetime == 1 ~ "low",
    freetime == 2 ~ "low",
    freetime == 3 ~ "low",
    freetime == 4 ~ "high",
    freetime == 5 ~"high"
  ))

data <- data %>%
  mutate(goout = case_when(
    goout == 1 ~ "low",
    goout == 2 ~ "low",
    goout == 3 ~ "low",
    goout == 4 ~ "high",
    goout == 5 ~"high"
  ))

data <- data %>%
  mutate(Dalc = case_when(
    Dalc == 1 ~ "low",
    Dalc == 2 ~ "low",
    Dalc == 3 ~ "low",
    Dalc == 4 ~ "high",
    Dalc == 5 ~"high"
  ))

data <- data %>%
  mutate(Walc = case_when(
    Walc == 1 ~ "low",
    Walc == 2 ~ "low",
    Walc == 3 ~ "low",
    Walc == 4 ~ "high",
    Walc == 5 ~"high"
  ))

data <- data %>%
  mutate(health = case_when(
    health == 1 ~ "low",
    health == 2 ~ "low",
    health == 3 ~ "low",
    health == 4 ~ "high",
    health == 5 ~"high"
  ))
```

Additionally, using information from the famsup and internet variables, we created a variable called "stable_learning_env". If famsup is "yes" and internet is "yes", then stable_learning_env is "yes", otherwise "no".

```{r}
data <- data %>%
  mutate(stable_learning_env = case_when(
    internet =="yes" & famsup =="yes" ~"yes",
    TRUE ~"no"
  ))
```

Also, we created a new variable "high_freq_absent", which if absences >= 10 for a student, we considered them a highly frequent student.

```{r}
data <- data %>%
  mutate(high_freq_absent = case_when(
    absences >= 10 ~"yes",
    TRUE ~"no"
  ))
```

We also created a "failed" variable, which was "yes" if failures > 0, and "no" otherwise.

```{r}
data <- data %>%
  mutate(failed = case_when(
    failures > 0 ~"yes",
    TRUE ~"no"
  ))
```



### Exploratory Data Analysis 

```{r}
summary(data)
```

First, I will start off with univariate and bivariate plots of the response variable and key predictors I see being important. 

```{r}
data %>%
  filter(failed =="yes") %>%
  ggplot(aes(G3)) + 
  geom_histogram(stat = "count") +
  labs(title="Final Grade Distribution")

data %>%
  filter(failed =="yes") %>%
  ggplot(aes(G3)) + 
  geom_histogram(stat = "count") +
  labs(title="Final Grade Distribution")

data %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()

data %>%
  keep(is.character) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram(stat="count")


```

Above we see that the response variable, G3, is pretty normally distributed, thus no transformation is necessary, 


```{r}
ggplot(data = data, aes(x = G3, y = first_gen_college, fill=first_gen_college)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = Walc, fill = Walc)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = famrel, fill = famrel)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y= sex, fill = sex)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y = high_freq_absent, fill = high_freq_absent)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=failed, fill = failed)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=romantic, fill = romantic)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=internet, fill = internet)) +
  geom_boxplot() 
ggplot(data = data, aes(x = G3, y=goout, fill = goout)) +
  geom_boxplot() 

```

From the initial explorations above, we can see a few possible trends. Students who had at least one of the following traits: failed a class previously, were a highly frequent absent student, frequently went out, without internet, were frequent drinkers on the weekend, were in romantic relationships, and were first generation students, on average had lower final grades than their counterparts.


```{r}
names(data)
```

```{r}
num_cols <- unlist(lapply(data, is.numeric))
quant_vars <- data[,num_cols]
cor(quant_vars)
#library(corr)
#quant_vars %>% correlate() %>% network_plot(min_cor=0.2)
```


## Splitting data into training and testing sets

```{r}
data <- data %>%
  mutate(ord_g3 = factor(G3, ordered=T)
  )
data$ord_g3
attach(data)
set.seed(3)
train_ind <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg <- -train_ind
training <- data[train_ind, ]
testing <- data[test_ind_neg, ]
training
testing

```


## Linear model

Modeling, diagnostics, predictions

```{r}
base_lm <- lm(G3 ~ . -G2 -G1 -ord_g3 -stable_learning_env, data)
summary(base_lm)

step.model <- stepAIC(base_lm, direction="both")
summary(step.model)
```

From fitting the base variables in a linear model, we can see that the variables sex, schoolsup, famsup, romantic, freetime, goout, absences, and failed are active. 

Based on the backwards stepwise regression model, it seems that the variables sex, Mjob, studytime, higher, romantic, freetime, goout, absences, first_gen_college, famsup, failed.

Based on these active variables, some interactions that we think could be significant are: schoolsup*failed, famsup*first_gen_college, higher*first_gen_college.

```{r}
activelm <- lm(G3 ~ (sex + schoolsup + romantic + freetime + goout + absences + failed + Mjob + studytime + higher + first_gen_college + famsup)^2)

summary(activelm)
```

From this, we can see that there seem to be significant interaction effects between sex and schoolsup, sex and first_gen_college, sex and Mjob, schoolsup and absences, schoolsup and studytime, schoolsup and first_gen_college, absences and failed, failed and first_gen_college, Mjob and studytime, Mjob and first_gen_college, studytime and famsup. 

## Linear model on the training data set

```{r}
base_lm_tr <- lm(G3 ~ . -G2 -G1 -ord_g3 -stable_learning_env, data = training)
step.model_tr <- stepAIC(base_lm_tr, direction="both")
AIC(step.model_tr)
summary(step.model_tr)


```
AIC of 1793.426 and adjusted R-squared of 0.2742.

Active variables: goout, absences, first_gen_college, failed, freetime, romantic, famsup, schoolsup, studytime, famsize, sex. Let's explore these with interaction terms.

```{r}
activelm_tr <- lm(G3 ~ (sex + schoolsup + romantic + freetime + goout + absences + failed + studytime + first_gen_college + famsup + famsize), data = training)
summary(activelm_tr)
AIC(activelm_tr)
```
AIC of 1801.82, Adjusted R-squared of 0.2389.

Adding all interaction terms:

```{r}
interlm_tr <- lm(G3 ~ (sex + schoolsup + romantic + freetime + goout + absences + failed + studytime + first_gen_college + famsup + famsize)^2, data = training)
summary(interlm_tr)
AIC(interlm_tr)

```
Significant interactions: absences*famsup, absences*failed schoolsup*first_gen_college, schoolsup*absences, schoolsup*studytime, sex*failed, sex*first_gen_college

```{r}
interlm_tr1 <- lm(G3 ~ (sex + schoolsup + romantic + freetime + goout + absences + failed + studytime + first_gen_college + famsup + famsize + absences*famsup + absences*failed + schoolsup*first_gen_college + schoolsup*absences + schoolsup*studytime + sex*failed + sex*first_gen_college), data = training)
summary(interlm_tr1)
AIC(interlm_tr1)

```
Schoolsup seems to have a high correlation with first_gen_college and studytime. Absences hasa high correlation with failed. Both of these make sense intuitively. Sex and first_Gen_college seems to have a high correlation, which does not make much sense intuitively and should be explored further.

Including only active variables and paring down the model:

```{r}
interlm_tr2 <- lm(G3 ~ (romantic + freetime + goout + failed + studytime + first_gen_college + famsup + famsize + absences*failed + schoolsup*first_gen_college + schoolsup*studytime + sex*first_gen_college), data = training)
summary(interlm_tr2)
AIC(interlm_tr2)

```
AIC of 1767.442, Adjusted R-squared of 0.3255.

Using the model on the testing set:

```{r}
pred.lm <- predict(interlm_tr2, testing)
pred.lm
mse_test <- mean((pred.lm - testing$G3)^2)
testing$G3
mse_test
```


## Multicategory ordinal logit model

Due to the way grades are assigned as values between 0 and 20, we would like to consider G3 as an ordered categorical variable with 21 levels. This would allow us to fit a multicategory ordinal logistic model to the data. 

We examine the EDA and active variables in the linear model to choose the predictors in our base model. 

Fitting the base model:

```{r}

require(foreign)
require(nnet)
require(ggplot2)
require(reshape2)
require(MASS)
require(Hmisc)

mod <-polr(ord_g3 ~ . -G1 -G2 -G3, data = training)
summary(mod)
acc.ord <- predict(mod, training)
ctable <- table(training$G3, acc.ord)
round((sum(diag(ctable))/sum(ctable))*100,2)
ctable

mod1 <- polr(ord_g3 ~ failed + high_freq_absent + romantic + internet + goout + first_gen_college + Walc + sex + schoolsup + famsup + absences + studytime + higher, data = training)
summary(mod1)
(ctable <- coef(summary(mod1)))
```

Calculate and store p-values:

```{r}
p1 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p1))

```
Confidence intervals for parameter estimates:

```{r}
(ci1 <- confint(mod1))

```

Analyzing the p-values and confidence intervals allows us to determine whether the coefficient estimates are significant. Based on these, failed, romantic, goout, first_gen_college, sex, schoolsup, famsup, studytime seem to be active. (Studytime is dubious, but we will include it in the next model)

Refitting a model with these predictors:


```{r}
mod2 <- polr(ord_g3 ~ failed + romantic + goout + first_gen_college + studytime + sex + schoolsup + famsup, data = training)
summary(mod2)
(ctable <- coef(summary(mod2)))
p2 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p2))
(ci2 <- confint(mod2))

```

AIC has decreased. 

Based on the p-values and confidence intervals, romantic does not seem to be significant. Let's try excluding it. 

Pared-down model again: 
```{r}
mod3 <- polr(ord_g3 ~ failed + goout + first_gen_college + sex + schoolsup + studytime, data = training, Hess=TRUE)
summary(mod3)
(ctable <- coef(summary(mod3)))
p3 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p3))
(ci3 <- confint(mod3))

```
All predictors are significant, but AIC has increased compared to mod2.

Evaluating accuracy of the model for the training set:

```{r}
acc.ord3 <- predict(mod3, training)
ctable <- table(training$G3, acc.ord3)
round((sum(diag(ctable))/sum(ctable))*100,2)
ctable
```

Very terrible accuracy even for the training set.

What if we add interaction terms?

Let's base our interaction terms on the discussion for the linear model. 

```{r}
mod4 <- polr(ord_g3 ~ failed + goout + romantic +  first_gen_college + sex + schoolsup + sex*schoolsup + sex*first_gen_college + schoolsup * failed + schoolsup * studytime + schoolsup * first_gen_college + studytime*famsup, data = training)
summary(mod4)
(ctable <- coef(summary(mod4)))
p4 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p4))
(ci4 <- confint(mod4))
```
AIC has decreased significantly compared to the previous models without interaction terms, by nearly 20. However, in this model, sex, its interaction with schoolsup, and its interaction with first_gen_college all seem to be insignificant. The interaction between studytime and famsup and failed and schoolsup do not seem significant either, so let us remove it to pare down the model: 

```{r}
mod5 <- polr(ord_g3 ~ failed + goout + romantic + schoolsup + first_gen_college + schoolsup * studytime + schoolsup * first_gen_college, data = training)
summary(mod5)
(ctable <- coef(summary(mod5)))
p5 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p5))
(ci5 <- confint(mod5))

```
This has resulted in an increase in the AIC, which is still lower than the first three models. 

Let's check the accuracy of this model with interaction terms:

```{r}
acc.ord4 <- predict(mod4, training)
ctable <- table(training$G3, acc.ord4)
round((sum(diag(ctable))/sum(ctable))*100,2)
ctable

```
The accuracy is even lower than mod3, at only 19.94% for the training set.


Checking on testing set:

```{r}
pred.ord3 <- predict(mod3, testing)
ctable <- table(testing$G3, pred.ord3)
round((sum(diag(ctable))/sum(ctable))*100,2)

pred.ord4 <- predict(mod4, testing)
ctable <- table(testing$G3, pred.ord4)
round((sum(diag(ctable))/sum(ctable))*100,2)


pred.ord5 <- predict(mod5, testing)
ctable <- table(testing$G3, pred.ord5)
round((sum(diag(ctable))/sum(ctable))*100,2)


```
Accuracy rates are even lower, at 8.86%, 11.39%, and 10.13%. 

Highly inaccurate model, not a good fit for the data.


## Recoding g3 with larger categories

```{r}
library(car)
data <- data %>%
  mutate(cat_g3 = case_when(
    G3 == 0 ~ "Poor",
    G3 <= 9 ~ "Weak",
    G3 <= 13 ~ "Sufficient",
    G3 <= 15 ~ "Good",
    G3 <= 17 ~"Very Good",
    G3 <= 20 ~ "Excellent"
  ))
data <- data %>%
  mutate(cat_g3 = factor(cat_g3, levels=c("Poor", "Weak", "Sufficient", "Good", "Very Good", "Excellent"), ordered=TRUE))


set.seed(3)
train_ind <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg <- -train_ind
ftrain <- data[train_ind, ]
ftest <- data[test_ind_neg, ]

```

Trying out a multicat ordinal logit on this:

```{r}
mod6 <- polr(cat_g3 ~ failed + goout + romantic + schoolsup + first_gen_college + schoolsup * studytime + schoolsup * first_gen_college, data = ftrain)
summary(mod6)
(ctable <- coef(summary(mod6)))
p6 <- pnorm(abs(ctable[, "t value"]), lower.tail = FALSE) * 2
(ctable <- cbind(ctable, "p value" = p6))
(ci5 <- confint(mod6))


acc.ord6 <- predict(mod6, ftrain)
ctable <- table(ftrain$cat_g3, acc.ord6)
ctable

```
Still not very accurate for the training

Random forest:

```{r}
library(randomForest)
rf.cat<-randomForest(cat_g3~. -G1 -G2 -G3 -ord_g3,data = ftrain,ntree=50, importance=TRUE) 
print(rf.cat)
importance(rf.cat)
varImpPlot(rf.cat)
rf.acc<- predict(rf.cat, ftrain, type = 'class')
t<-table(predictions=rf.acc, actual=ftrain$cat_g3)
t
sum(diag(t))/sum(t)

```

Very fitted model with accuracy for training data >99%.

Let's see what the accuracy rate for the testing set is:

```{r}
rf.pred<- predict(rf.cat, ftest, type = 'class')
t<-table(predictions=rf.pred, actual=ftest$cat_g3)
t
sum(diag(t))/sum(t)

```
39.2% accuracy, which is an improvement, but a low value.


Let's choose the most important variables, as well as interaction effects we believe to be important based on previous exploration:

```{r}
rf.cat1<-randomForest(cat_g3~failed + absences+ Mjob + studytime + goout + romantic + schoolsup + first_gen_college + schoolsup * studytime + schoolsup * first_gen_college,data = ftrain, ntree=50, importance=TRUE) 
print(rf.cat1)
importance(rf.cat1)
varImpPlot(rf.cat1)
rf.acc<- predict(rf.cat1, ftrain, type = 'class')
t<-table(predictions=rf.acc, actual=ftrain$cat_g3)
t
sum(diag(t))/sum(t)

```
Much worse fit for the data, 50.32% OOB estimate of error rate and 70.25% accuracy rate for the training data.

```{r}
rf.pred1<- predict(rf.cat1, ftest, type = 'class')
t<-table(predictions=rf.pred1, actual=ftest$cat_g3)
t
sum(diag(t))/sum(t)

```
38% Accuracy, which is less than the previous RF model.


## Recoding binary response variable for pass-fail

Considering final grades as a continuous variable and ordinal categorical variable gave poor results. Therefore, we'd like to model a binary variable that indicates whether the student passes (grade >= 10) or fails (<10). 

```{r}
data <- data %>%
  mutate(pf = case_when(
    G3 >= 10 ~ "pass",
    G3 < 10 ~ "fail"
  ))
data <- data %>%
  mutate(pf = factor(pf, levels=c("pass", "fail"), ordered = FALSE))

set.seed(3)
train_ind <- sample(x = nrow(data), size = 0.8 * nrow(data))
test_ind_neg <- -train_ind
ftrain <- data[train_ind, ]
ftest <- data[test_ind_neg, ]

```

## Fitting a decision tree on pass-fail

```{r}
library(tree)
library(rpart)

```


## Fitting random forest on pass-fail 

Fitting with ALL predictors: 

```{r}
rf.bin<-randomForest(pf~. -G1 -G2 -G3 -ord_g3 - cat_g3 -failures -reason -health -age -nursery,data = ftrain,mtry=3, ntree=50, importance=TRUE) 
print(rf.bin)
importance(rf.bin)
varImpPlot(rf.bin)
rf.acc<- predict(rf.bin, ftrain, type = 'class')
t<-table(predictions=rf.acc, actual=ftrain$cat_g3)
t
sum(diag(t))/sum(t)

```
Predictions on testing set:
```{r}
rf.pred2<- predict(rf.bin, ftest, type = 'class')
t<-table(predictions=rf.pred2, actual=ftest$pf)
t
sum(diag(t))/sum(t)

```
72.15% accuracy rate.

Finding the best random forest model by including important predictors:

```{r}
rf.bin<-randomForest(pf~failed + absences+ guardian + studytime + goout + schoolsup + first_gen_college + internet,data = ftrain,mtry=3, ntree=50, importance=TRUE) 
print(rf.bin)
importance(rf.bin)
varImpPlot(rf.bin)
rf.acc<- predict(rf.bin, ftrain, type = 'class')
t<-table(predictions=rf.acc, actual=ftrain$cat_g3)
t
sum(diag(t))/sum(t)

```
The pared-down model has an 00B estimate of error rate of 25.95% and a training set prediction accuracy rate of 89.6%.

Predictions on testing set:

```{r}
rf.pred3<- predict(rf.bin, ftest, type = 'class')
t<-table(predictions=rf.pred3, actual=ftest$pf)
t
sum(diag(t))/sum(t)

```

72.15% prediction accuracy rate.

Overall the random-forests for pass-fail indicate that the most important factors affecting whether the student passes/fails are failed, absences, guardian, studytime, goout, schoolsup, first_gen_college, internet.